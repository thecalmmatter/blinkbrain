# blinkbrain
In a blink, it gets the brain. BlinkBrain turns videos into smart conversations.

1. initialize_embedder() Function

	•	Purpose: Initializes the SentenceTransformer model used for embedding text. The model chosen in this code is 'all-distilroberta-v1', which is a lightweight version of the RoBERTa transformer model, suitable for generating dense vector representations (embeddings) of sentences.
	•	Details: This function is cached to improve performance and avoid reloading the model each time. It is decorated with @st.cache_resource to ensure that the model is loaded only once during the session.
	•	Application: Useful for text embedding tasks like semantic search, clustering, or other natural language processing (NLP) tasks.

2. get_or_create_collection() Function

	•	Purpose: This function interacts with ChromaDB to either fetch an existing collection of embeddings or create a new collection if it does not already exist.
	•	Details: The function handles the case where the collection name might already exist by catching the UniqueConstraintError and returning the existing collection.
	•	Application: Ensures the app can store and retrieve text embeddings from the database for later use, such as querying for relevant context in Q&A sessions.

3. call_llama_finetune() Function

	•	Purpose: This function fine-tunes the transcribed text using the Llama model via the Ollama API.
	•	Details: The function takes the transcribed text as input, constructs a prompt to refine it, and returns the processed output. The model is instructed to make appropriate punctuation adjustments and correct grammar while preserving the original dialogue.
	•	Application: Essential for improving the quality of the transcribed text before storing embeddings or providing it to the user in the Q&A process.

4. add_embeddings_to_chromadb() Function

	•	Purpose: This function generates sentence embeddings for the fine-tuned transcription text and stores them in ChromaDB for future use.
	•	Details: The sentences are first split, then encoded using the SentenceTransformer, and stored with unique identifiers and metadata in the ChromaDB collection.
	•	Application: Enables semantic search by storing fine-tuned text embeddings that can later be retrieved based on user queries.

5. get_relevant_context_from_chromadb() Function

	•	Purpose: Given a user query, this function retrieves the most relevant sentences from the stored embeddings in ChromaDB by comparing the query embedding to the stored document embeddings.
	•	Details: The function computes the cosine similarity between the query and stored embeddings and retrieves the top k most relevant sentences from the collection.
	•	Application: Provides relevant context to the Llama model in the Q&A section of the app, enhancing the quality of answers.

6. Video File Upload Section

	•	Purpose: Allows the user to upload a video file for transcription.
	•	Details: The video is saved temporarily, displayed in the Streamlit app, and then processed by the transcription function. The uploaded file is handled securely and deleted after processing.
	•	Application: Allows the core functionality of video-to-text conversion to be triggered by user input.

7. Transcription Section

	•	Purpose: Uses the vid2cleantxt package to transcribe the uploaded video.
	•	Details: The transcription model, openai/whisper-small.en, is used to convert speech from the video into text. The transcription result is displayed in the app for review.
	•	Application: Central to the app’s functionality, enabling the conversion of video content into editable text.

8. Fine-tuning Section

	•	Purpose: Fine-tunes the transcribed text using the Llama model, improving grammar and readability.
	•	Details: The fine-tuned text is processed using the Llama model via the Ollama API, and the refined text is displayed to the user.
	•	Application: Ensures that the transcribed text is of high quality, improving its clarity and usability in the Q&A section.

9. Q&A Section

	•	Purpose: Allows users to ask questions based on the fine-tuned transcription, with answers generated by the Llama model.
	•	Details: The system retrieves relevant context from ChromaDB based on the user query and passes it to Llama for generating the answer.
	•	Application: Provides interactive capabilities, allowing users to engage with the transcribed content and extract useful information.

10. File Download Section

	•	Purpose: Enables the user to download the fine-tuned transcription as a text file.
	•	Details: The fine-tuned transcription is made available for download as a .txt file. This allows users to keep a copy of the processed text.
	•	Application: Useful for users who wish to save or further analyze the fine-tuned transcription.

11. Cleanup Section

	•	Purpose: Removes the uploaded video file after processing.
	•	Details: The uploaded file is securely deleted after the necessary processing is completed, ensuring the app doesn’t store unnecessary files.
	•	Application: Keeps the app environment clean and reduces unnecessary data storage.

Relevant Literature for Models Used

	1.	SentenceTransformer:
	•	This is a transformer-based model that allows for efficient sentence-level embeddings. It leverages the power of models like BERT, RoBERTa, and DistilBERT for creating dense vector representations of sentences.
	•	Source: Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.
	2.	Llama (via Ollama):
	•	Llama is a powerful language model that can be fine-tuned for a variety of NLP tasks. It is particularly used in this app for text refinement and generating contextually relevant responses.
	•	Source: Research on Llama models is continually evolving, with major contributions from open-source projects like OpenAI’s GPT-3 and other transformer architectures.
	3.	Whisper:
	•	Whisper is a general-purpose speech recognition model developed by OpenAI, designed to transcribe spoken language into text. It’s optimized for high accuracy and supports multiple languages and audio conditions.
	•	Source: Radford, A., et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of NeurIPS 2021.
	4.	ChromaDB:
	•	ChromaDB is a database for handling document embeddings, providing capabilities for storing, querying, and managing vector embeddings, which is useful for semantic search.
	•	Source: Chroma is an open-source vector database optimized for machine learning and AI applications.

TODOs

Suggestions for New Features and Applications

	1.	Multilingual Support:
	•	Add support for transcribing and fine-tuning videos in multiple languages. This would open the app to a global audience and enhance accessibility.
	2.	Advanced Search and Filtering:
	•	Integrate more advanced filtering options in the Q&A section, such as categorizing questions by topics or filtering by keywords in the transcribed text.
	3.	Real-Time Transcription:
	•	Implement real-time transcription for live videos or streaming content. This could benefit educational content creators or journalists who need immediate transcription.
	4.	Voice Interaction:
	•	Allow users to ask questions via voice, integrating speech-to-text functionality for a more hands-free, interactive experience.
	5.	Content Summarization:
	•	Add a feature for summarizing long transcriptions. This would allow users to quickly get a gist of the video content without reading everything.
	6.	Searchable Video Library:
	•	Create a video library where users can upload multiple videos, store them, and search through the transcriptions with relevant context available.
	7.	Integration with Learning Management Systems (LMS):
	•	Integrate the app with educational platforms like Moodle or Canvas to provide an easy way for students and instructors to access transcriptions and interact with course materials.

These new features could enhance the usability and reach of the application, especially in educational or professional environments.
